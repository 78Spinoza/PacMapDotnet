# PACMAP Implementation in C++: Full Summary Guide

## Overall Description
This guide details a complete, efficient, deterministic PACMAP (Pairwise Controlled Manifold Approximation and Projection) implementation in C++, reusing 90-95% of an existing UMAP codebase with HNSW for ANN. PACMAP reduces high-dimensional data to low-dim embeddings using triplet-based optimization: neighbors (local attraction), mid-near (global attraction), far pairs (repulsion). It employs a three-phase process with dynamic mid-near weights (high-to-low for global-to-local focus) and Adam optimizer for stable, fast convergence (preferred over GD for adaptive rates, momentum, and robustness to noisy gradients; deterministic with seeded RNG). Key features: HNSW for efficient sampling/transform, seeded RNG for reproducibility, OpenMP for det-safe parallelism, model persistence (save/load params, triplets, data, HNSW with CRC32 validation). No PCA init; random seeded normal. Matches Python ref; add transform via weighted NN interpolation. Build with CMake; test against Iris/MNIST.

## Step-by-Step Implementation

### Step 1: Project Setup and Model Structure
Fork UMAP to "PACMAPCpp". Update CMakeLists.txt: Set project "PACMAP", lib "pacmap", add new sources (`pacmap_triplet_sampling.cpp`, `pacmap_gradient.cpp`, `pacmap_optimization.cpp`, `pacmap_transform.cpp`, `pacmap_persistence.cpp`). Rename UMAP files to "pacmap_*" (e.g., `pacmap_model.h/cpp`, `pacmap_simple_wrapper.h/cpp`, `pacmap_distance.h/cpp`). Reuse HNSW, progress, quantization utils.

In `pacmap_model.h`:
```cpp
#pragma once
#include <vector>
#include <memory>
#include <random>
#include <hnswlib/hnswlib.h>

enum PacMapMetric { PACMAP_METRIC_EUCLIDEAN = 0, PACMAP_METRIC_COSINE = 1 /* etc. */ };
enum TripletType { NEIGHBOR, MID_NEAR, FURTHER };

struct Triplet {
    int anchor, neighbor;
    TripletType type;
    float weight = 1.0f;
};

struct PacMapModel {
    int n_samples = 0, n_features = 0, n_components = 2, n_neighbors = 10;
    float mn_ratio = 0.5f, fp_ratio = 2.0f, learning_rate = 1.0f;
    int phase1_iters = 100, phase2_iters = 100, phase3_iters = 250;
    PacMapMetric metric = PACMAP_METRIC_EUCLIDEAN;
    int random_seed = -1;  // -1: non-det; else seeded
    bool use_quantization = false;
    int hnsw_m = 16, hnsw_ef_construction = 200, hnsw_ef_search = 200;

    std::vector<Triplet> triplets;
    std::vector<float> training_data, embedding;
    std::unique_ptr<hnswlib::HierarchicalNSW<float>> original_space_index;

    float min_embedding_dist = 0, p95_embedding_dist = 0 /* safety stats */;

    std::mt19937 rng;  // Seeded in fit
};
```
In `pacmap_model.cpp`: Add `PacMapModel* pacmap_create() { return new PacMapModel(); }`, `void pacmap_destroy(PacMapModel* model) { delete model; }`.

### Step 2: Distance Metrics
Rename `uwot_distance.h/cpp` to `pacmap_distance.h/cpp`. Reuse functions like `float compute_distance(float* a, float* b, int dim, PacMapMetric metric)`. Add validation: `void validate_metric_data(const float* data, int n_obs, int n_dim, PacMapMetric metric)` (console warn for invalid data, e.g., non-binary Hamming). Vectorize with SIMD for efficiency.

### Step 3: Triplet Sampling
New file `pacmap_triplet_sampling.h/cpp`. Efficient HNSW ANN for neighbors; seeded random with dist filters for mid-near/far. Threshold: n_obs > 8000 use approx.

```cpp
// Helper
std::mt19937 get_seeded_rng(int seed) {
    return seed >= 0 ? std::mt19937(seed) : std::mt19937(std::random_device{}());
}

// Main
void sample_triplets(PacMapModel* model, float* data, uwot_progress_callback_v2 callback) {
    // Normalize/quantize (reuse UMAP)
    std::vector<float> normalized_data(/* process data */);
    model->training_data = normalized_data;  // Save for transform

    // Build HNSW (reuse UMAP)
    model->original_space_index = /* build_hnsw(normalized_data, n_features, hnsw_m, ef_construction) */;

    model->rng = get_seeded_rng(model->random_seed);

    // Neighbors
    std::vector<Triplet> neighbor_triplets;
    #pragma omp parallel for  // Det-safe
    for (int i = 0; i < model->n_samples; ++i) {
        auto knn = model->original_space_index->searchKnn(normalized_data.data() + i * model->n_features, model->n_neighbors + 1);
        for (size_t j = 1; j < knn.size(); ++j) {  // Skip self
            neighbor_triplets.emplace_back(Triplet{i, static_cast<int>(knn[j].second), NEIGHBOR});
        }
    }

    // Mid-near: Sample mid-range dists
    int n_mn = static_cast<int>(model->n_neighbors * model->mn_ratio);
    std::vector<Triplet> mn_triplets;
    std::uniform_int_distribution<int> dist(0, model->n_samples - 1);
    float p25_dist = /* sample 1000 pairs, compute 25th percentile */, p75_dist = /* 75th */;
    for (int i = 0; i < model->n_samples * n_mn * 2; /* oversample for uniqueness */) {  // Adjust loop
        int anchor = dist(model->rng), neigh = dist(model->rng);
        if (anchor == neigh) continue;
        float d = compute_distance(/* points */, model->n_features, model->metric);
        if (d >= p25_dist && d <= p75_dist) mn_triplets.emplace_back(Triplet{anchor, neigh, MID_NEAR});
        if (mn_triplets.size() >= model->n_samples * n_mn) break;
    }

    // Far: Sample > p75_dist
    int n_fp = static_cast<int>(model->n_neighbors * model->fp_ratio);
    std::vector<Triplet> far_triplets;  // Similar loop, filter d > p75_dist

    // Combine
    model->triplets = std::move(neighbor_triplets);
    model->triplets.insert(model->triplets.end(), mn_triplets.begin(), mn_triplets.end());
    model->triplets.insert(model->triplets.end(), far_triplets.begin(), far_triplets.end());

    callback("Sampling Triplets", /* progress */);
}
```

### Step 4: Weight Schedule and Gradients
New file `pacmap_gradient.h/cpp`.

```cpp
std::tuple<float, float, float> get_weights(int current_iter, int total_iters) {
    float progress = static_cast<float>(current_iter) / total_iters;
    float w_n = 1.0f, w_f = 1.0f, w_mn;
    if (progress < 0.1f) w_mn = 1000.0f * (1.0f - progress * 10.0f) + 3.0f * (progress * 10.0f);
    else if (progress < 0.4f) w_mn = 3.0f;
    else w_mn = 3.0f * (1.0f - (progress - 0.4f) / 0.6f);
    return {w_n, w_mn, w_f};
}

void compute_gradients(const std::vector<float>& embedding, const std::vector<Triplet>& triplets,
                       std::vector<float>& gradients, float w_n, float w_mn, float w_f, int n_components) {
    gradients.assign(embedding.size(), 0.0f);
    #pragma omp parallel for
    for (const auto& t : triplets) {
        size_t idx_a = static_cast<size_t>(t.anchor) * n_components;
        size_t idx_n = static_cast<size_t>(t.neighbor) * n_components;
        std::vector<float> diff(n_components);
        float d_ij = 0.0f;
        for (int d = 0; d < n_components; ++d) {
            diff[d] = embedding[idx_a + d] - embedding[idx_n + d];
            d_ij += diff[d] * diff[d];
        }
        d_ij = std::sqrt(std::max(d_ij, 1e-8f));

        float grad_mag;
        switch (t.type) {
            case NEIGHBOR: grad_mag = w_n * 10.0f / std::pow(10.0f + d_ij, 2.0f); break;
            case MID_NEAR: grad_mag = w_mn * 10000.0f / std::pow(10000.0f + d_ij, 2.0f); break;
            case FURTHER: grad_mag = -w_f / std::pow(1.0f + d_ij, 2.0f); break;
        }

        float scale = grad_mag / d_ij;
        for (int d = 0; d < n_components; ++d) {
            float g = scale * diff[d];
            #pragma omp atomic
            gradients[idx_a + d] += g;
            gradients[idx_n + d] -= g;
        }
    }
}
```

### Step 5: Three-Phase Optimization with Adam
New file `pacmap_optimization.h/cpp`. Adam preferred for stability (adaptive lr, momentum; deterministic, efficient, parallel-friendly; converges faster than GD).

```cpp
void optimize_embedding(PacMapModel* model, float* embedding_out, uwot_progress_callback_v2 callback) {
    std::vector<float> embedding(model->n_samples * model->n_components);
    std::normal_distribution<float> norm(0.0f, 1e-4f);  // Seeded random init
    for (auto& val : embedding) val = norm(model->rng);

    int total_iters = model->phase1_iters + model->phase2_iters + model->phase3_iters;
    std::vector<float> gradients(embedding.size()), m(embedding.size(), 0.0f), v(embedding.size(), 0.0f);
    float beta1 = 0.9f, beta2 = 0.999f, eps = 1e-8f;

    for (int iter = 0; iter < total_iters; ++iter) {
        auto [w_n, w_mn, w_f] = get_weights(iter, total_iters);
        compute_gradients(embedding, model->triplets, gradients, w_n, w_mn, w_f, model->n_components);

        float beta1_pow = std::pow(beta1, iter + 1), beta2_pow = std::pow(beta2, iter + 1);
        #pragma omp parallel for
        for (size_t i = 0; i < embedding.size(); ++i) {
            m[i] = beta1 * m[i] + (1 - beta1) * gradients[i];
            v[i] = beta2 * v[i] + (1 - beta2) * (gradients[i] * gradients[i]);
            float m_hat = m[i] / (1 - beta1_pow), v_hat = v[i] / (1 - beta2_pow);
            embedding[i] -= model->learning_rate * m_hat / (std::sqrt(v_hat) + eps);
        }

        std::string phase = (iter < model->phase1_iters) ? "Phase 1: Global" :
                            (iter < model->phase1_iters + model->phase2_iters) ? "Phase 2: Balance" : "Phase 3: Local";
        callback(phase.c_str(), iter, total_iters, static_cast<float>(iter) / total_iters * 100.0f, nullptr);
    }

    model->embedding = embedding;  // Save for transform
    // Compute safety stats (dist percentiles in embed space, reuse UMAP)
    std::memcpy(embedding_out, embedding.data(), embedding.size() * sizeof(float));
}
```

### Step 6: Fit API
In `pacmap_simple_wrapper.cpp`:
```cpp
int pacmap_fit_with_progress_v2(PacMapModel* model, float* data, int n_obs, int n_dim, int embedding_dim,
                                int n_neighbors, float mn_ratio, float fp_ratio,
                                int phase1_iters, int phase2_iters, int phase3_iters,
                                PacMapMetric metric, float* embedding, uwot_progress_callback_v2 callback,
                                int force_exact_knn, int M, int ef_construction, int ef_search,
                                int use_quantization, int random_seed, int autoHNSWParam) {
    // Set model params
    model->n_samples = n_obs; model->n_features = n_dim; model->n_components = embedding_dim;
    model->n_neighbors = n_neighbors; model->mn_ratio = mn_ratio; model->fp_ratio = fp_ratio;
    model->phase1_iters = phase1_iters; /* etc. */;
    model->metric = metric; model->random_seed = random_seed; model->use_quantization = use_quantization;
    model->hnsw_m = M; /* etc. */;

    // Normalize/quantize data (reuse UMAP)
    sample_triplets(model, data, callback);
    optimize_embedding(model, embedding, callback);
    // Build embed HNSW for transform safety (reuse UMAP)
    return UWOT_SUCCESS;
}
```

### Step 7: Transform
New file `pacmap_transform.h/cpp`. Weighted NN interpolation.
```cpp
int pacmap_transform_detailed(PacMapModel* model, float* new_data, int n_new_obs, int n_dim,
                              float* embedding, int* nn_indices, float* nn_distances,
                              float* confidence_score, int* outlier_level /* etc. */) {
    // Normalize/quantize new_data
    #pragma omp parallel for
    for (int i = 0; i < n_new_obs; ++i) {
        std::vector<float> point(/* from new_data */);
        auto knn = model->original_space_index->searchKnn(point.data(), model->n_neighbors);
        std::vector<float> weights(knn.size());
        float sum_w = 0.0f, min_dist = knn[0].first;
        for (size_t j = 0; j < knn.size(); ++j) {
            weights[j] = 1.0f / (std::max(knn[j].first, 1e-8f));
            sum_w += weights[j];
            if (nn_indices) nn_indices[i * model->n_neighbors + j] = knn[j].second;
            if (nn_distances) nn_distances[i * model->n_neighbors + j] = knn[j].first;
        }
        for (auto& w : weights) w /= sum_w;

        for (int d = 0; d < model->n_components; ++d) {
            float coord = 0.0f;
            for (size_t j = 0; j < knn.size(); ++j)
                coord += weights[j] * model->embedding[knn[j].second * model->n_components + d];
            embedding[i * model->n_components + d] = coord;
        }

        // Safety (reuse UMAP): confidence = 1 - (min_dist - min_embed) / (p95 - min_embed)
    }
    return UWOT_SUCCESS;
}
```

### Step 8: Persistence
New file `pacmap_persistence.h/cpp`. Reuse UMAP save/load with CRC32. New items: triplets (serialize vector: size, then anchor/neigh/type/weight), phase iters, ratios, seed, safety stats. Load validates CRC mismatches.

```cpp
int pacmap_save_model(PacMapModel* model, const char* filename) {
    // Open file, write params (n_samples etc.), triplets (uint32 size, then array), training_data, embedding, HNSW (reuse UMAP serialization), CRC32.
    // Compute CRC for data/embedding/version.
    return UWOT_SUCCESS;
}

PacMapModel* pacmap_load_model(const char* filename) {
    // Read params, triplets, data, embedding, HNSW. Validate CRC. Return new model or nullptr on error.
}
```

### Step 9: Utilities and Testing
Add `int pacmap_get_model_info(/* output params */)` (reuse pattern). Tests: Unit (sampling dists, weights, gradients match Python); Integration (fit Iris seeded, compare embeds); Benchmarks (time/memory, det checks). Use Catch2/Google Test.


Based on a review of the implementation guide, web search results on PACMAP risks/pitfalls (e.g., misleading visualizations, hyperparam sensitivity, preprocessing effects, convergence issues in DR methods), and the Rust implementation (which lacks persistence/transform entirely but notes efficiency via ANN/SIMD/parallel and reproducibility via seeding), here's a thorough analysis:

### Did We Miss Anything?
Overall, the guide is comprehensive and covers the core algorithm faithfully (triplet sampling, phased Adam optimization, transform via interpolation), with strong reuse of UMAP infrastructure. However, we missed a few minor but practical elements for robustness, especially in production/C++ contexts:

1. **Error Handling and Edge Cases**:
   - Missed: Explicit error codes/handling in functions (e.g., UWOT_ERROR_* from UMAP for invalid n_obs < n_neighbors, dim mismatches, NaN in data). Add `pacmap_get_error_message(int code)` (reuse UMAP).
   - Missed: Edge tests/cases like n_obs=1 (degenerate), high dims (>1000, memory risk), or metric-specific failures (e.g., zero-norm in cosine).
   - Handle: Integrate in fit/transform (e.g., return UWOT_ERROR_INVALID_PARAMS if !model->is_fitted for transform).

2. **Floating-Point Precision and Cross-Platform Determinism**:
   - Missed: FP ops can vary by hardware/compiler (e.g., fused multiply-add), breaking det. Risks from search: Non-det across platforms.
   - Handle: Use `-ffp-contract=off` in CMake for consistent FP. Document: Same hardware/seeds ensure det; add model flag for strict FP mode.

3. **Quantization/Compression Integration**:
   - Partially missed: Guide mentions optional use_quantization but not full integration in sampling/transform (e.g., quantize during HNSW build).
   - Handle: In sample_triplets/transform, apply quantization if enabled (reuse UMAP pq_centroids).

4. **Hyperparam Validation/Tuning**:
   - Missed: Auto-tuning (e.g., n_neighbors based on n_obs, like Rust auto-scale). Risks from search: Poor defaults lead to bad embeddings (false clusters).
   - Handle: Add warnings in validate_metric_data for extreme ratios/iters. Test sensitivity.

5. **Performance/Parallel Risks**:
   - Missed: OpenMP atomic overhead in gradients (potential bottleneck for large triplets).
   - Handle: Already det-safe; profile and fallback to serial if threads>cores. Add runtime threshold for parallel (like Rust approx_threshold).

6. **Documentation/Tests**:
   - Missed: Full README with usage, risks (e.g., "Projections may mislead clusters" from search), benchmarks.
   - Handle: Expand tests for risks (e.g., memory leak checks with Valgrind).

We handled major risks well:
- **Memory Explosion (Triplets)**: Vectors + oversampling limits; search notes large datasets risk this, but HNSW mitigates.
- **Convergence Mismatch**: Adam + phased weights; search highlights sensitivity, but Adam's adaptivity addresses.
- **HNSW Inaccuracy**: ef boost/exact fallback; reproducible with seeds.
- **Determinism**: Seeded RNG/parallel; matches Rust.
- **Efficiency**: HNSW/parallel/SIMD; better than Rust (multi-metrics, transform).

No critical algorithm misses (e.g., matches Python/Rust phases/loss). Add above for completeness—low effort.

### Transform Weights: Saved/Loaded?
No, and they shouldn't be—these are runtime-computed interpolation weights (inverse distances from new points to training NN). They depend on query data, so not savable (like UMAP). Model saves what's needed for computation: training_data, embedding, HNSW (for NN queries), params (n_neighbors for k). On load, transform recomputes weights identically (det with seeded HNSW). Risks: None missed; search shows DR transforms often recompute (e.g., UMAP). If needed, add optional precompute for fixed queries (not standard).Based on a review of the implementation guide, web search results on PACMAP risks/pitfalls (e.g., misleading visualizations, hyperparam sensitivity, preprocessing effects, convergence issues in DR methods), and the Rust implementation (which lacks persistence/transform entirely but notes efficiency via ANN/SIMD/parallel and reproducibility via seeding), here's a thorough analysis:

### Did We Miss Anything?
Overall, the guide is comprehensive and covers the core algorithm faithfully (triplet sampling, phased Adam optimization, transform via interpolation), with strong reuse of UMAP infrastructure. However, we missed a few minor but practical elements for robustness, especially in production/C++ contexts:

1. **Error Handling and Edge Cases**:
   - Missed: Explicit error codes/handling in functions (e.g., UWOT_ERROR_* from UMAP for invalid n_obs < n_neighbors, dim mismatches, NaN in data). Add `pacmap_get_error_message(int code)` (reuse UMAP).
   - Missed: Edge tests/cases like n_obs=1 (degenerate), high dims (>1000, memory risk), or metric-specific failures (e.g., zero-norm in cosine).
   - Handle: Integrate in fit/transform (e.g., return UWOT_ERROR_INVALID_PARAMS if !model->is_fitted for transform).

2. **Floating-Point Precision and Cross-Platform Determinism**:
   - Missed: FP ops can vary by hardware/compiler (e.g., fused multiply-add), breaking det. Risks from search: Non-det across platforms.
   - Handle: Use `-ffp-contract=off` in CMake for consistent FP. Document: Same hardware/seeds ensure det; add model flag for strict FP mode.

3. **Quantization/Compression Integration**:
   - Partially missed: Guide mentions optional use_quantization but not full integration in sampling/transform (e.g., quantize during HNSW build).
   - Handle: In sample_triplets/transform, apply quantization if enabled (reuse UMAP pq_centroids).

4. **Hyperparam Validation/Tuning**:
   - Missed: Auto-tuning (e.g., n_neighbors based on n_obs, like Rust auto-scale). Risks from search: Poor defaults lead to bad embeddings (false clusters).
   - Handle: Add warnings in validate_metric_data for extreme ratios/iters. Test sensitivity.

5. **Performance/Parallel Risks**:
   - Missed: OpenMP atomic overhead in gradients (potential bottleneck for large triplets).
   - Handle: Already det-safe; profile and fallback to serial if threads>cores. Add runtime threshold for parallel (like Rust approx_threshold).

6. **Documentation/Tests**:
   - Missed: Full README with usage, risks (e.g., "Projections may mislead clusters" from search), benchmarks.
   - Handle: Expand tests for risks (e.g., memory leak checks with Valgrind).

We handled major risks well:
- **Memory Explosion (Triplets)**: Vectors + oversampling limits; search notes large datasets risk this, but HNSW mitigates.
- **Convergence Mismatch**: Adam + phased weights; search highlights sensitivity, but Adam's adaptivity addresses.
- **HNSW Inaccuracy**: ef boost/exact fallback; reproducible with seeds.
- **Determinism**: Seeded RNG/parallel; matches Rust.
- **Efficiency**: HNSW/parallel/SIMD; better than Rust (multi-metrics, transform).

No critical algorithm misses (e.g., matches Python/Rust phases/loss). Add above for completeness—low effort.

### Transform Weights: Saved/Loaded?
No, and they shouldn't be—these are runtime-computed interpolation weights (inverse distances from new points to training NN). They depend on query data, so not savable (like UMAP). Model saves what's needed for computation: training_data, embedding, HNSW (for NN queries), params (n_neighbors for k). On load, transform recomputes weights identically (det with seeded HNSW). Risks: None missed; search shows DR transforms often recompute (e.g., UMAP). If needed, add optional precompute for fixed queries (not standard).




