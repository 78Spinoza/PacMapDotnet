use ndarray::Array2;
use pacmap::{Configuration, fit_transform, PairConfiguration};
use crate::pairs::{get_knn_indices};
#[cfg(feature = "use_hnsw")]
use hnsw_rs::{hnsw::Hnsw, dist::DistL2};
use ndarray::Array1;
use std::sync::atomic::{AtomicBool, Ordering};
// Removed unused HashSet import

/// Global verbose toggle to control print statements
static VERBOSE_MODE: AtomicBool = AtomicBool::new(false);

/// Enable or disable verbose logging
pub fn set_verbose(enabled: bool) {
    VERBOSE_MODE.store(enabled, Ordering::Relaxed);
}

/// Check if verbose mode is enabled
pub fn is_verbose() -> bool {
    VERBOSE_MODE.load(Ordering::Relaxed) || std::env::var("PACMAP_VERBOSE").is_ok()
}

/// Conditional print macro - only prints if verbose mode is enabled
macro_rules! vprint {
    ($($arg:tt)*) => {
        if is_verbose() {
            eprintln!($($arg)*);
        }
    };
}
use crate::quantize::{quantize_embedding};
use half::f16;
use crate::serialization::{PaCMAP, DistanceStats, PacMAPConfig, TransformStats, load_hnsw_from_serialized, custom_serialize_hnsw, serialize_hnsw_to_bytes, deserialize_hnsw_from_bytes};
use crc32fast::Hasher;
use bytemuck;
use crate::stats::{compute_distance_stats, NormalizationParams, NormalizationMode, recommend_normalization_mode};
use crate::hnsw_params::HnswParams;
// use crate::recall_validation::validate_hnsw_quality_with_retry;
mod pairs;
mod quantize;
pub mod serialization;
pub mod stats;
pub mod hnsw_params;
pub mod ffi;
pub mod error;
#[cfg(test)]
mod test_normalization;
#[cfg(test)]
mod test_hnsw_params;
#[cfg(test)]
mod test_ffi;
#[cfg(test)]
mod test_working_hnsw;
mod recall_validation;

/// Fit the data using HNSW-accelerated PaCMAP transformation
pub fn fit_transform_hnsw(data: Array2<f64>, config: Configuration, force_exact_knn: bool, progress_callback: Option<&(dyn Fn(&str, usize, usize, f32, &str) + Send + Sync)>) -> Result<(Array2<f64>, Option<HnswParams>), Box<dyn std::error::Error>> {
    // Call the extended version with auto-scale HNSW parameters
    fit_transform_hnsw_with_params(data, config, force_exact_knn, progress_callback, None, true)
}

/// Fit the data using HNSW-accelerated PaCMAP transformation with custom HNSW parameters
pub fn fit_transform_hnsw_with_params(data: Array2<f64>, config: Configuration, force_exact_knn: bool, progress_callback: Option<&(dyn Fn(&str, usize, usize, f32, &str) + Send + Sync)>, custom_hnsw_params: Option<HnswParams>, autodetect_hnsw_params: bool) -> Result<(Array2<f64>, Option<HnswParams>), Box<dyn std::error::Error>> {
    use std::time::Instant;
    let start_time = Instant::now();
    let n_neighbors = config.override_neighbors.unwrap_or(10);
    let seed = config.seed.unwrap_or(42);
    let (n_samples, _) = data.dim();

    // Determine whether to use HNSW or fall back to standard PaCMAP
    // Force exact KNN overrides the size threshold
    let use_hnsw = !force_exact_knn && n_samples > 1000;

    // DEBUG: Report actual parameters via callback
    let debug_msg = format!(" DLL DEBUG: force_exact_knn={}, n_samples={}, use_hnsw={}", force_exact_knn, n_samples, use_hnsw);
    if let Some(callback) = progress_callback {
        callback("DLL Debug", 5, 100, 5.0, &debug_msg);
    }

    // Helper function to call enhanced progress callback with timing
    let report_progress = |phase: &str, current: usize, total: usize, percent: f32, message: &str| {
        if let Some(callback) = progress_callback {
            let elapsed = start_time.elapsed();
            let enhanced_message = format!("{} ({})", message, format_duration(elapsed));
            callback(phase, current, total, percent, &enhanced_message);
        }
    };

    // Track optimized HNSW parameters to return them for model storage
    let mut optimized_hnsw_params: Option<HnswParams> = None;

    let updated_config = if use_hnsw {
        vprint!(" DEBUG: use_hnsw=true, force_exact_knn={}, n_samples={}", force_exact_knn, n_samples);
        vprint!(" Using HNSW-accelerated neighbor search for {} samples", n_samples);
        report_progress("KNN Method", 25, 100, 25.0, "Using HNSW for fast approximate neighbor search");

        // Use custom HNSW parameters or auto-scale for validation and optimization
        let (_, n_features) = data.dim();
        let mut hnsw_params = if let Some(custom_params) = custom_hnsw_params {
            vprint!("DEBUG: Using custom HNSW parameters: M={}, ef_construction={}, ef_search={}",
                    custom_params.m, custom_params.ef_construction, custom_params.ef_search);
            custom_params
        } else {
            vprint!("DEBUG: Auto-scaling HNSW parameters for {} samples, {} features", n_samples, n_features);
            HnswParams::auto_scale(n_samples, n_features, n_neighbors)
        };

        // Conditionally validate HNSW recall quality with auto-retry and ef_search optimization
        if autodetect_hnsw_params {
            vprint!("DEBUG: HNSW autodetection enabled - performing recall validation and parameter optimization");
            match crate::recall_validation::validate_hnsw_quality_with_retry_and_params(data.view(), n_neighbors, seed, hnsw_params.ef_search, Some(&report_progress), Some(hnsw_params.clone())) {
                Ok((quality_msg, optimized_ef_search)) => {
                    // Update ef_search with optimized value from retry mechanism
                    if optimized_ef_search != hnsw_params.ef_search {
                        vprint!("HNSW ef_search optimized: {} -> {} (auto-retry tuning)", hnsw_params.ef_search, optimized_ef_search);
                        hnsw_params.ef_search = optimized_ef_search;
                    }
                    vprint!("HNSW validation result: {}", quality_msg);
                },
                Err(error) => {
                    return Err(format!("HNSW validation failed: {}", error).into());
                }
            }
        } else {
            vprint!("DEBUG: HNSW autodetection disabled - using provided parameters as-is");
            report_progress("HNSW Skip", 35, 100, 35.0, "Skipping recall validation - using provided HNSW parameters");
        }

        // Store optimized parameters for model storage
        optimized_hnsw_params = Some(hnsw_params.clone());

        // FIXED: Single HNSW neighbor computation with per-point symmetrization (eliminates redundancy)
        vprint!("DEBUG: Starting optimized HNSW neighbor computation with ef_search={}...", hnsw_params.ef_search);
        let hnsw_pairs = match crate::pairs::compute_pairs_hnsw_to_per_point_with_params(data.view(), n_neighbors, seed, Some(hnsw_params.clone())) {
            Ok(mut nn_per_point) => {
                vprint!("DEBUG: Applying symmetric per-point merging...");
                symmetrize_per_point(&mut nn_per_point, n_neighbors);

                // Convert back to pairs format with guaranteed size
                let mut pairs = Vec::with_capacity(n_samples * n_neighbors);
                for (i, neighbors) in nn_per_point.iter().enumerate() {
                    for &(j, _dist) in neighbors {
                        pairs.push((i, j));
                    }
                }
                vprint!("DEBUG: Single-pass HNSW computation completed: {} pairs", pairs.len());
                pairs
            },
            Err(e) => {
                vprint!("WARNING: Per-point HNSW failed ({}), falling back to basic pairs computation", e);
                // Fallback: compute basic pairs without per-point distances (still only one HNSW call)
                crate::pairs::compute_pairs_hnsw_with_params(data.view(), n_neighbors, seed, Some(hnsw_params.clone()))
            }
        };


        // Handle HNSW pair count - truncate or warn if mismatch
        let expected_pairs = n_samples * n_neighbors;
        let actual_pairs = hnsw_pairs.len();

        if actual_pairs != expected_pairs {
            vprint!("WARNING:  HNSW pair count mismatch: expected {} ({}Ã—{}), got {} - adjusting",
                   expected_pairs, n_samples, n_neighbors, actual_pairs);
        }

        // Check if HNSW returned sufficient pairs (at least 90% of expected)
        let min_acceptable_pairs = (expected_pairs as f32 * 0.9) as usize;

        if actual_pairs < min_acceptable_pairs {
            vprint!("WARNING: HNSW returned insufficient pairs ({} < {}), falling back to exact KNN", actual_pairs, min_acceptable_pairs);
            // Force exact KNN by using default PairConfiguration (not HNSW-based)
            Configuration {
                pair_configuration: PairConfiguration::default(),  // Force exact KNN - no HNSW
                ..config
            }
        } else {

        // Convert pairs to required format: Array2<u32> with shape (expected_pairs, 2)
        let mut pair_neighbors = Array2::<u32>::zeros((expected_pairs, 2));

        // Fill with available pairs, truncated to expected count
        for (idx, &(i, j)) in hnsw_pairs.iter().take(expected_pairs).enumerate() {
            pair_neighbors[[idx, 0]] = i as u32;
            pair_neighbors[[idx, 1]] = j as u32;
        }

        vprint!("SUCCESS: HNSW pairs validated: using {} pairs for PaCMAP (truncated from {})", expected_pairs, actual_pairs);

        // Create configuration with precomputed neighbors
        Configuration {
            pair_configuration: PairConfiguration::NeighborsProvided { pair_neighbors },
            ..config
        }
        }
    } else {
        vprint!(" DEBUG: use_hnsw=false, force_exact_knn={}, n_samples={}", force_exact_knn, n_samples);
        if force_exact_knn {
            vprint!(" Using exact KNN search for {} samples (forced by user)", n_samples);
            report_progress("Exact KNN", 25, 100, 25.0, "SUCCESS: EXACT KNN ENABLED - Using O(n^2) brute-force neighbor search (precise)");
        } else {
            vprint!(" Using exact KNN search for {} samples (small dataset: auto-fallback)", n_samples);
            report_progress("Exact KNN", 25, 100, 25.0, "SUCCESS: EXACT KNN AUTO - Using O(n^2) exact search (dataset <1000 samples)");
        }
        // FORCE exact KNN by using default PairConfiguration (not HNSW-based)
        Configuration {
            pair_configuration: PairConfiguration::default(),  // Force exact KNN - no HNSW
            ..config
        }
    };

    // Convert data to f32 for PaCMAP
    let data_f32 = data.mapv(|v| v as f32);

    // Perform PaCMAP dimensionality reduction with HNSW neighbors (if computed)
    // Run fit_transform (no epoch-level progress available from external pacmap crate)
    let (embedding_f32, _) = fit_transform(data_f32.view(), updated_config)?;

    // Convert embedding back to f64 for the public API
    let embedding_f64 = embedding_f32.mapv(|v| v as f64);
    Ok((embedding_f64, optimized_hnsw_params))
}

/// Progress callback function type for Rust usage
pub type ProgressCallback = Box<dyn Fn(&str, usize, usize, f32, &str) + Send + Sync>;

/// Enhanced fit function with normalization, HNSW auto-scaling, and progress reporting with force_exact_knn control
/// This version properly normalizes data and auto-scales HNSW parameters for optimal performance
pub fn fit_transform_normalized_with_progress_and_force_knn(
    data: Array2<f64>,
    config: Configuration,
    normalization_mode: Option<NormalizationMode>,
    progress_callback: Option<ProgressCallback>,
    force_exact_knn: bool,
    use_quantization: bool
) -> Result<(Array2<f64>, PaCMAP), Box<dyn std::error::Error>> {
    // Call the extended version with auto-scale HNSW parameters
    fit_transform_normalized_with_progress_and_force_knn_with_hnsw(
        data, config, normalization_mode, progress_callback,
        force_exact_knn, use_quantization, None, true // Enable autodetect by default
    )
}

/// Enhanced fit function with normalization, HNSW configuration, and progress reporting
/// This version accepts FFI HNSW parameters or falls back to auto-scaling
pub fn fit_transform_normalized_with_progress_and_force_knn_with_hnsw(
    mut data: Array2<f64>,
    config: Configuration,
    normalization_mode: Option<NormalizationMode>,
    progress_callback: Option<ProgressCallback>,
    force_exact_knn: bool,
    use_quantization: bool,
    ffi_hnsw_params: Option<HnswParams>,
    autodetect_hnsw_params: bool
) -> Result<(Array2<f64>, PaCMAP), Box<dyn std::error::Error>> {
    let progress = |phase: &str, current: usize, total: usize, percent: f32, message: &str| {
        if let Some(ref callback) = progress_callback {
            callback(phase, current, total, percent, message);
        }
    };

    progress("Initializing", 0, 100, 0.0, "Preparing dataset for PacMAP fitting");
    let (n_samples, n_features) = data.dim();
    let seed = config.seed.unwrap_or(42);

    // Validate input data
    progress("Validating", 1, 100, 1.0, "Validating input data");
    crate::error::validate_data_shape(&data.view(), 2, 1)?;
    crate::error::validate_neighbors(n_samples, config.override_neighbors.unwrap_or(10))?;

    // Determine normalization mode (auto-detect if not specified)
    progress("Analyzing", 5, 100, 5.0, "Analyzing data characteristics for normalization");
    let norm_mode = normalization_mode.unwrap_or_else(|| recommend_normalization_mode(&data));

    // Initialize and fit normalization parameters
    progress("Normalizing", 10, 100, 10.0, &format!("Applying {:?} normalization", norm_mode));
    let mut normalization = NormalizationParams::new(n_features, norm_mode);
    if norm_mode != NormalizationMode::None {
        normalization.fit_transform(&mut data)?;
    }

    // Use FFI HNSW parameters or auto-scale based on dataset characteristics (only if not forcing exact KNN)
    let n_neighbors = config.override_neighbors.unwrap_or(10);
    let hnsw_params = if !force_exact_knn {
        if let Some(ffi_params) = ffi_hnsw_params {
            progress("HNSW Config", 20, 100, 20.0, "Using FFI-provided HNSW parameters");
            vprint!("DEBUG: Using FFI HNSW parameters: M={}, ef_construction={}, ef_search={}",
                    ffi_params.m, ffi_params.ef_construction, ffi_params.ef_search);
            ffi_params
        } else {
            progress("HNSW Config", 20, 100, 20.0, "Auto-scaling HNSW parameters for dataset");
            HnswParams::auto_scale(n_samples, n_features, n_neighbors)
        }
    } else {
        progress("Exact KNN", 20, 100, 20.0, "Skipping HNSW configuration - using exact KNN");
        HnswParams::default() // Not used, but needed for struct
    };

    // Log HNSW parameter selection for user information (only if using HNSW)
    if !force_exact_knn {
        let characteristics = hnsw_params.get_characteristics();
        let hnsw_message = format!("HNSW: M={}, ef_construction={}, ef_search={}",
                                   hnsw_params.m, hnsw_params.ef_construction, hnsw_params.ef_search);
        progress("HNSW Ready", 25, 100, 25.0, &hnsw_message);

        vprint!("DEBUG: Auto-scaled HNSW parameters for {}k samples, {} features:", n_samples / 1000, n_features);
        vprint!("   M={}, ef_construction={}, ef_search={}",
                  hnsw_params.m, hnsw_params.ef_construction, hnsw_params.ef_search);
        vprint!("   {}", characteristics);
    } else {
        progress("Exact KNN Ready", 25, 100, 25.0, "Exact KNN configuration complete - high precision mode");
        vprint!("DEBUG: Using exact KNN for {} samples, {} features (high precision mode)", n_samples, n_features);
    }

    // Perform fit using HNSW-enhanced PaCMAP on normalized data
    progress("Embedding", 30, 100, 30.0, "Computing PacMAP embedding (this may take time for large datasets)");
    let callback_ref = progress_callback.as_ref().map(|cb| cb.as_ref());
    let (embedding, optimized_hnsw_params) = fit_transform_hnsw_with_params(data.clone(), config.clone(), force_exact_knn, callback_ref, Some(hnsw_params.clone()), autodetect_hnsw_params)?;
    progress("Embedding Done", 80, 100, 80.0, "PacMAP embedding computation completed");

    // Compute statistics over the embedding for outlier detection
    progress("Finalizing", 90, 100, 90.0, "Computing embedding statistics and building model");
    let (mean, p95, max) = compute_distance_stats(&embedding, seed);

    // Compute the centroid of the embedding for "No Man's Land" detection
    let embedding_centroid = if embedding.len() > 0 {
        Some(embedding.mean_axis(ndarray::Axis(0)).unwrap())
    } else {
        None
    };

    // Create serializable config with optimized HNSW parameters (if available)
    let final_hnsw_params = optimized_hnsw_params.unwrap_or(hnsw_params);
    let total_epochs = config.num_iters.0 + config.num_iters.1 + config.num_iters.2;
    let pacmap_config = PacMAPConfig {
        n_neighbors,
        embedding_dim: config.embedding_dimensions,
        n_epochs: total_epochs,
        learning_rate: config.learning_rate as f64,
        mid_near_ratio: config.mid_near_ratio as f64,
        far_pair_ratio: config.far_pair_ratio as f64,
        seed: config.seed,
        hnsw_params: final_hnsw_params,
        used_hnsw: !force_exact_knn, // Track whether HNSW was used
        force_knn: force_exact_knn, // Track whether KNN was forced
    };

    // Build complete model struct with normalization parameters
    let mut model = PaCMAP {
        embedding: embedding.clone(),
        config: pacmap_config,
        stats: DistanceStats {
            mean_distance: mean,
            p95_distance: p95,
            max_distance: max
        },
        normalization, // Store fitted normalization parameters
        quantize_on_save: use_quantization,
        quantized_embedding: None,
        original_data: None,
        fitted_projections: embedding.clone(), // ALWAYS save exact fitted projections
        embedding_centroid, // Store centroid for "No Man's Land" detection
        #[cfg(feature = "use_hnsw")]
        hnsw_index: None, // Will be populated during index building
        #[cfg(feature = "use_hnsw")]
        embedding_hnsw_index: None, // Will be populated during index building
        serialized_hnsw_index: None, // Will be populated for massive datasets
        // REMOVED: Never save transformed space HNSW index
        // serialized_embedding_hnsw_index: None, // REMOVED - always rebuild for accuracy
        hnsw_index_crc32: None, // Will be populated when HNSW indexes are serialized
        fitted_projections_crc32: None, // Will be populated with fitted projections checksum
    };

    // Store transform data: original input and fitted projections
    // This enables proper 2-stage neighbor search for new points
    model.store_transform_data(&data, &embedding);

    // Build and store HNSW indices for fast transform operations
    if !force_exact_knn && n_samples >= 1000 {
        #[cfg(feature = "use_hnsw")]
        {
            vprint!("Building HNSW indices for fast transform operations...");
            progress("Building Indices", 95, 100, 95.0, "Building HNSW indices for fast transforms");

            const MASSIVE_DATASET_THRESHOLD: usize = 500_000; // 500k points

            // Build original data HNSW index
            let original_data = model.get_original_data().unwrap();
            let (n_orig_samples, _) = original_data.dim();
            let hnsw_params = &model.config.hnsw_params;

            if n_orig_samples > MASSIVE_DATASET_THRESHOLD {
                // For massive datasets, serialize the index for fast loading
                vprint!("[Package] Massive dataset detected: serializing HNSW indices for fast loading");
                let max_layer = ((n_orig_samples as f32).ln() / (hnsw_params.m as f32).ln()).ceil() as usize + 1;
                let max_layer = max_layer.min(32).max(4);

                let original_index = custom_serialize_hnsw(&original_data, hnsw_params, max_layer)?;
                let embedding_index = custom_serialize_hnsw(&embedding, hnsw_params, max_layer)?;

                let original_bytes = serialize_hnsw_to_bytes(&original_index)?;
                let _embedding_bytes = serialize_hnsw_to_bytes(&embedding_index)?;

                // Calculate CRC32 checksums
                let mut hasher = Hasher::new();
                hasher.update(&original_bytes);
                let original_crc = hasher.finalize();

                // Calculate CRC32 for fitted projections (critical data integrity)
                let mut hasher = Hasher::new();
                let projection_data = model.fitted_projections.as_slice().unwrap();
                hasher.update(bytemuck::cast_slice::<f64, u8>(projection_data));
                let fitted_projections_crc = hasher.finalize();

                model.serialized_hnsw_index = Some(original_bytes);
                // model.serialized_embedding_hnsw_index = Some(embedding_bytes); // REMOVED
                model.hnsw_index_crc32 = Some(original_crc);
                model.fitted_projections_crc32 = Some(fitted_projections_crc); // New integrity check

                // For massive datasets, don't store the raw quantized data to save space
                model.original_data = None;
                vprint!("[OK] HNSW indices serialized for massive dataset ({} points) with CRC checksums", n_orig_samples);
            } else {
                // For normal datasets, build indices in memory AND serialize them for persistence
                vprint!("[BUILD] Building HNSW indices in memory for normal dataset");

                // Build original data index
                use hnsw_rs::hnsw::Hnsw;
                use hnsw_rs::dist::DistL2;

                let points: Vec<Vec<f32>> = (0..n_orig_samples)
                    .map(|i| original_data.row(i).iter().map(|&x| x as f32).collect())
                    .collect();

                let max_layer = ((n_orig_samples as f32).ln() / (hnsw_params.m as f32).ln()).ceil() as usize + 1;
                let max_layer = max_layer.min(32).max(4);

                let hnsw = Hnsw::<f32, DistL2>::new(
                    hnsw_params.m,
                    n_orig_samples,
                    max_layer,
                    hnsw_params.ef_construction,
                    DistL2{}
                );

                let data_with_id: Vec<(&[f32], usize)> = points.iter().enumerate().map(|(i, p)| (p.as_slice(), i)).collect();

                #[cfg(feature = "parallel")]
                {
                    hnsw.parallel_insert(&data_with_id);
                }
                #[cfg(not(feature = "parallel"))]
                {
                    for (point, i) in data_with_id {
                        hnsw.insert((&point.to_vec(), i));
                    }
                }

                model.hnsw_index = Some(hnsw);

                // Build embedding index
                let embedding_points: Vec<Vec<f32>> = (0..n_samples)
                    .map(|i| embedding.row(i).iter().map(|&x| x as f32).collect())
                    .collect();

                let embedding_hnsw = Hnsw::<f32, DistL2>::new(
                    hnsw_params.m,
                    n_samples,
                    max_layer,
                    hnsw_params.ef_construction,
                    DistL2{}
                );

                let embedding_data_with_id: Vec<(&[f32], usize)> = embedding_points.iter().enumerate().map(|(i, p)| (p.as_slice(), i)).collect();

                #[cfg(feature = "parallel")]
                {
                    embedding_hnsw.parallel_insert(&embedding_data_with_id);
                }
                #[cfg(not(feature = "parallel"))]
                {
                    for (point, i) in embedding_data_with_id {
                        embedding_hnsw.insert((&point.to_vec(), i));
                    }
                }

                model.embedding_hnsw_index = Some(embedding_hnsw);

                // CRITICAL FIX: Also serialize normal datasets for persistence across saves/loads
                vprint!("[SAVE] Serializing HNSW indices for persistence (normal dataset)");
                let original_index = custom_serialize_hnsw(&original_data, hnsw_params, max_layer)?;
                let embedding_index = custom_serialize_hnsw(&embedding, hnsw_params, max_layer)?;

                let original_bytes = serialize_hnsw_to_bytes(&original_index)?;
                let _embedding_bytes = serialize_hnsw_to_bytes(&embedding_index)?;

                // Calculate CRC32 checksums
                let mut hasher = Hasher::new();
                hasher.update(&original_bytes);
                let original_crc = hasher.finalize();

                // Calculate CRC32 for fitted projections (critical data integrity)
                let mut hasher = Hasher::new();
                let projection_data = model.fitted_projections.as_slice().unwrap();
                hasher.update(bytemuck::cast_slice::<f64, u8>(projection_data));
                let fitted_projections_crc = hasher.finalize();

                model.serialized_hnsw_index = Some(original_bytes);
                // model.serialized_embedding_hnsw_index = Some(embedding_bytes); // REMOVED
                model.hnsw_index_crc32 = Some(original_crc);
                model.fitted_projections_crc32 = Some(fitted_projections_crc); // New integrity check

                vprint!("[OK] HNSW indices built in memory AND serialized for persistence ({} points) with fitted projections CRC", n_orig_samples);
            }
        }
    }

    progress("Complete", 100, 100, 100.0, "PacMAP fitting completed successfully with HNSW indices");

    // Display final model settings
    model.print_model_settings("Fitted Model");

    Ok((embedding, model))
}

/// Enhanced fit function with normalization, HNSW auto-scaling, and progress reporting
/// This version properly normalizes data and auto-scales HNSW parameters for optimal performance
pub fn fit_transform_normalized_with_progress(
    data: Array2<f64>,
    config: Configuration,
    normalization_mode: Option<NormalizationMode>,
    progress_callback: Option<ProgressCallback>
) -> Result<(Array2<f64>, PaCMAP), Box<dyn std::error::Error>> {
    // Call the extended version with force_exact_knn = false (use HNSW by default)
    fit_transform_normalized_with_progress_and_force_knn(data, config, normalization_mode, progress_callback, false, false)
}

/// Enhanced fit function with normalization and HNSW auto-scaling
/// This version properly normalizes data and auto-scales HNSW parameters for optimal performance
pub fn fit_transform_normalized(
    data: Array2<f64>,
    config: Configuration,
    normalization_mode: Option<NormalizationMode>
) -> Result<(Array2<f64>, PaCMAP), Box<dyn std::error::Error>> {
    // Call the progress version without a callback (silent mode)
    fit_transform_normalized_with_progress(data, config, normalization_mode, None)
}

/// Legacy function - now calls the enhanced version with auto-normalization
pub fn fit_transform_quantized(data: Array2<f64>, config: Configuration) -> Result<(Array2<f16>, PaCMAP), Box<dyn std::error::Error>> {
    let (embedding, model) = fit_transform_normalized(data, config, None)?;

    // Quantize embedding to f16 for compactness
    let quantized = quantize_embedding(&embedding);
    Ok((quantized, model))
}

/// Transform new data using a fitted model with consistent normalization
/// This is critical - must use the same normalization as training data
/// Optimize transform positions using gradient descent with PacMAP forces
/// This implements the missing optimization loop for proper transform quality
fn optimize_transform_positions(
    initial_positions: &mut Array2<f64>,
    new_data: &Array2<f64>,
    original_data: &Array2<f64>,
    fitted_projections: &Array2<f64>,
    config: &PacMAPConfig,
    initial_neighbors: &[Vec<usize>]
) -> Result<Array2<f64>, Box<dyn std::error::Error>> {
    let (n_new_points, embedding_dim) = initial_positions.dim();
    let n_fitted_points = fitted_projections.shape()[0];

    // Optimization parameters (reduced from full training for efficiency)
    let n_optimization_epochs = 50; // Much less than full training (450)
    let initial_learning_rate = 0.1;  // Smaller learning rate for fine-tuning
    let min_learning_rate = 0.01;

    vprint!("DEBUG: Starting transform optimization: {} epochs, {} new points, {} fitted points",
           n_optimization_epochs, n_new_points, n_fitted_points);

    // Create a combined embedding with both fitted points and new points
    let total_points = n_fitted_points + n_new_points;
    let mut combined_embedding = Array2::<f64>::zeros((total_points, embedding_dim));

    // Copy fitted projections (these are fixed during optimization)
    for i in 0..n_fitted_points {
        for j in 0..embedding_dim {
            combined_embedding[[i, j]] = fitted_projections[[i, j]];
        }
    }

    // Copy initial new point positions (these will be optimized)
    for i in 0..n_new_points {
        for j in 0..embedding_dim {
            combined_embedding[[n_fitted_points + i, j]] = initial_positions[[i, j]];
        }
    }

    // Optimization loop using gradient descent with attractive/repulsive forces
    for epoch in 0..n_optimization_epochs {
        // Decay learning rate over time
        let progress = epoch as f64 / n_optimization_epochs as f64;
        let learning_rate = initial_learning_rate * (1.0 - progress) + min_learning_rate * progress;

        // Compute forces for each new point
        for i in 0..n_new_points {
            let point_idx = n_fitted_points + i;
            let mut force = Array1::<f64>::zeros(embedding_dim);

            // Check if this point is identical to any training point - skip optimization if so
            let mut is_identical = false;
            for training_idx in 0..original_data.shape()[0] {
                let orig_dist = euclidean_distance(new_data.row(i), original_data.row(training_idx));
                if orig_dist < 1e-3 {
                    is_identical = true;
                    break;
                }
            }

            if is_identical {
                // Skip optimization for identical points - keep them at their exact fitted positions
                vprint!("[DEBUG] Optimization: Skipping optimization for point {} (identical to training data)", i);
                continue;
            }

            // Attractive forces to neighbors in original space
            for &neighbor_idx in initial_neighbors[i].iter().take(config.n_neighbors) {
                if neighbor_idx < n_fitted_points {
                    // Compute original space distance
                    let orig_dist = euclidean_distance(new_data.row(i), original_data.row(neighbor_idx));

                    // Compute embedding space distance
                    let mut emb_dist_sq = 0.0;
                    for d in 0..embedding_dim {
                        let diff = combined_embedding[[point_idx, d]] - combined_embedding[[neighbor_idx, d]];
                        emb_dist_sq += diff * diff;
                    }
                    let emb_dist = emb_dist_sq.sqrt().max(1e-8);

                    // Attractive force proportional to original distance mismatch
                    let target_dist = orig_dist * 0.1; // Scale down for 2D space
                    let force_magnitude = (target_dist - emb_dist) / emb_dist;

                    for d in 0..embedding_dim {
                        let direction = (combined_embedding[[neighbor_idx, d]] - combined_embedding[[point_idx, d]]) / emb_dist;
                        force[d] += force_magnitude * direction * 2.0; // Attractive force
                    }
                }
            }

            // Repulsive forces from randomly sampled points (prevent crowding)
            let n_repulsive_samples = 5; // Small number for efficiency
            for _ in 0..n_repulsive_samples {
                let random_idx = (epoch * n_new_points + i * 7) % n_fitted_points; // Deterministic "random"

                let mut emb_dist_sq = 0.0;
                for d in 0..embedding_dim {
                    let diff = combined_embedding[[point_idx, d]] - combined_embedding[[random_idx, d]];
                    emb_dist_sq += diff * diff;
                }
                let emb_dist = emb_dist_sq.sqrt().max(1e-8);

                // Repulsive force inversely proportional to distance
                let force_magnitude = 0.1 / (emb_dist * emb_dist + 1e-8);

                for d in 0..embedding_dim {
                    let direction = (combined_embedding[[point_idx, d]] - combined_embedding[[random_idx, d]]) / emb_dist;
                    force[d] += force_magnitude * direction; // Repulsive force
                }
            }

            // Apply force with learning rate (update new point position)
            // But only if this point is not identical to a training point
            let mut is_identical = false;
            for training_idx in 0..original_data.shape()[0] {
                let orig_dist = euclidean_distance(new_data.row(i), original_data.row(training_idx));
                if orig_dist < 1e-3 {
                    is_identical = true;
                    break;
                }
            }

            if !is_identical {
                for d in 0..embedding_dim {
                    combined_embedding[[point_idx, d]] += learning_rate * force[d];
                }
            }
        }

        // Progress reporting every 10 epochs
        if epoch % 10 == 0 {
            vprint!("DEBUG: Transform optimization epoch {}/{}, learning_rate: {:.4}", epoch, n_optimization_epochs, learning_rate);
        }
    }

    // Extract optimized positions for new points
    let mut optimized_positions = Array2::<f64>::zeros((n_new_points, embedding_dim));
    for i in 0..n_new_points {
        for j in 0..embedding_dim {
            optimized_positions[[i, j]] = combined_embedding[[n_fitted_points + i, j]];
        }
    }

    vprint!("[OK] Transform optimization completed: {} epochs, refined {} new point positions",
           n_optimization_epochs, n_new_points);

    Ok(optimized_positions)
}

pub fn transform_with_model(model: &mut PaCMAP, mut new_data: Array2<f64>) -> Result<Array2<f64>, Box<dyn std::error::Error>> {
    let (n_samples, n_features) = new_data.dim();

    // Validate feature dimensions match training data
    if n_features != model.normalization.n_features {
        return Err(format!(
            "Feature dimension mismatch: expected {}, got {}",
            model.normalization.n_features, n_features
        ).into());
    }

    // Apply the same normalization used during training
    if model.normalization.mode != NormalizationMode::None {
        model.normalization.transform(&mut new_data)?;
    }

    // Check if we have transform data available
    vprint!("[SEARCH] Transform: Checking for transform data...");
    let original_data = match model.get_original_data() {
        Some(data) => {
            vprint!("[OK] Transform: Original data found: {}x{}", data.shape()[0], data.shape()[1]);
            data
        }
        None => {
            vprint!("[ERROR] Transform: Original data NOT found - model may not have transform support");
            return Err("No original training data stored for transforms".into());
        }
    };
    // STAGE 1: Find initial neighbors in original high-dimensional space using stored HNSW index
    let k_initial = (model.config.n_neighbors * 3).max(50); // More candidates for better coverage

    // Ensure HNSW index is available BEFORE borrowing other data
    // Check both training flag and available serialized data for loaded models
    let needs_hnsw = model.config.used_hnsw || model.serialized_hnsw_index.is_some();
    if needs_hnsw {
        #[cfg(feature = "use_hnsw")]
        {
            model.ensure_hnsw_index()?;
        }
        #[cfg(not(feature = "use_hnsw"))]
        {
            return Err("HNSW index required but HNSW feature not enabled".into());
        }
    }

    let fitted_projections = {
        let projections = &model.fitted_projections;
        vprint!("[OK] Transform: Fitted projections found: {}x{}", projections.shape()[0], projections.shape()[1]);
        projections
    };

    let initial_neighbors: Vec<Vec<usize>> = if needs_hnsw {
        #[cfg(feature = "use_hnsw")]
        {

            if let Some(ref hnsw_index) = model.hnsw_index {
                // FAST PATH: Use stored HNSW index for original space neighbor search
                vprint!("[FAST] Using stored original data HNSW index for fast neighbor search");

                (0..n_samples).map(|i| {
                    let query: Vec<f32> = new_data.row(i).iter().map(|&x| x as f32).collect();
                    let neighbors = hnsw_index.search(&query, k_initial, model.config.hnsw_params.ef_search);
                    neighbors.into_iter().map(|n| n.d_id).collect::<Vec<usize>>()
                }).collect()
            } else {
                return Err("HNSW index required but not available after ensure_hnsw_index()".into());
            }
        }
        #[cfg(not(feature = "use_hnsw"))]
        {
            return Err("HNSW index required but HNSW feature not enabled".into());
        }
    } else {
        // EXACT KNN PATH: For models trained without HNSW
        vprint!("[SEARCH] Using exact KNN neighbor search for transform (brute-force)");

        // Find k_initial nearest neighbors in original high-dimensional space using exact search
        let initial_neighbors: Vec<Vec<usize>> = (0..n_samples).map(|i| {
            let mut distances_with_indices: Vec<(usize, f64)> = (0..original_data.shape()[0])
                .map(|j| {
                    let dist = euclidean_distance(new_data.row(i), original_data.row(j));
                    (j, dist)
                })
                .collect();

            // Sort by distance and take k_initial nearest neighbors
            distances_with_indices.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap());
            distances_with_indices
                .into_iter()
                .take(k_initial)
                .map(|(idx, _)| idx)
                .collect()
        }).collect();
        initial_neighbors
    };

    // STAGE 2: Project new points to 2D space using initial 3D neighbors
    let embedding_dim = fitted_projections.shape()[1];
    let mut transformed = Array2::zeros((n_samples, embedding_dim));

    for (i, neighbor_indices) in initial_neighbors.iter().enumerate() {
        // Check if this new point is identical to any training point by checking ALL training points
        let mut identical_training_point = None;
        let mut min_distance = f64::INFINITY;
        let mut closest_point = None;

        for training_idx in 0..original_data.shape()[0] {
            let orig_dist = euclidean_distance(new_data.row(i), original_data.row(training_idx));

            // Track the closest point
            if orig_dist < min_distance {
                min_distance = orig_dist;
                closest_point = Some(training_idx);
            }

            // PERFECT MATCH: Point is identical to training point (with tolerance for floating-point precision)
            if orig_dist < 1e-3 {
                identical_training_point = Some(training_idx);
                break;
            }
        }

        vprint!("[DEBUG] Transform: Point {} closest training point is {} with distance {:.12e}",
                i, closest_point.unwrap_or(0), min_distance);

        // If identical point found, use its exact fitted projection
        if let Some(identical_idx) = identical_training_point {
            vprint!("[DEBUG] Transform: Point {} identical to training point {}, using exact fitted projection", i, identical_idx);
            for j in 0..embedding_dim {
                transformed[[i, j]] = fitted_projections[[identical_idx, j]];
            }
            continue;
        }

        // Otherwise, use weighted interpolation based on distances in original space
        let mut weighted_position = Array1::<f64>::zeros(embedding_dim);
        let mut total_weight = 0.0;

        for &neighbor_idx in neighbor_indices.iter().take(model.config.n_neighbors) {
            if neighbor_idx < fitted_projections.shape()[0] {
                // Calculate inverse distance weight in original space
                let orig_dist = euclidean_distance(new_data.row(i), original_data.row(neighbor_idx));
                let weight = 1.0 / (orig_dist + 1e-8); // Avoid division by zero

                // Add weighted contribution from neighbor's projection
                for j in 0..embedding_dim {
                    weighted_position[j] += weight * fitted_projections[[neighbor_idx, j]];
                }
                total_weight += weight;
            }
        }

        // Normalize by total weight
        if total_weight > 0.0 {
            for j in 0..embedding_dim {
                transformed[[i, j]] = weighted_position[j] / total_weight;
            }
        }
    }

    // STAGE 3: Optimization loop to refine the initial projections
    // This was missing in the previous implementation!
    vprint!("[SEARCH] Transform STAGE 3: Optimizing projected positions with gradient descent");

    let optimized_transformed = optimize_transform_positions(
        &mut transformed,
        &new_data,
        &original_data,
        fitted_projections,
        &model.config,
        &initial_neighbors
    )?;

    // STAGE 4: Find final neighbors in the 2D embedding space
    // This is what the user actually wants - neighbors in the compressed space!
    vprint!("[SEARCH] Transform STAGE 4: Finding neighbors in 2D embedding space");

    let _final_neighbors = find_neighbors_in_embedding_space(&optimized_transformed, fitted_projections, model.config.n_neighbors, model)?;

    vprint!("[OK] Transform completed: 2D projection with optimization and neighbors in embedding space");

    Ok(optimized_transformed)
}

/// Enhanced transform function that returns both coordinates AND neighbors in 2D space
/// This is what users typically want: where the point projects to + what it's near
pub fn transform_with_neighbors(model: &PaCMAP, new_data: Array2<f64>) -> Result<(Array2<f64>, Vec<Vec<usize>>), Box<dyn std::error::Error>> {
    let (n_samples, n_features) = new_data.dim();

    // Validate feature dimensions match training data
    if n_features != model.normalization.n_features {
        return Err(format!(
            "Feature dimension mismatch: expected {}, got {}",
            model.normalization.n_features, n_features
        ).into());
    }

    // Apply the same normalization used during training
    let mut normalized_data = new_data;
    if model.normalization.mode != NormalizationMode::None {
        model.normalization.transform(&mut normalized_data)?;
    }

    // Check if we have transform data available
    vprint!("[SEARCH] Transform: Checking for transform data...");
    let original_data = match model.get_original_data() {
        Some(data) => {
            vprint!("[OK] Transform: Original data found: {}x{}", data.shape()[0], data.shape()[1]);
            data
        }
        None => {
            vprint!("[ERROR] Transform: Original data NOT found - model may not have transform support");
            return Err("No original training data stored for transforms".into());
        }
    };
    let fitted_projections = {
        let projections = &model.fitted_projections;
        vprint!("[OK] Transform: Fitted projections found: {}x{}", projections.shape()[0], projections.shape()[1]);
        projections
    };

    // STAGE 1: Find initial neighbors in original high-dimensional space
    let k_initial = (model.config.n_neighbors * 3).max(50);
    let _initial_neighbors = find_neighbors_in_original_space(&normalized_data, &original_data, k_initial, model)?;

    // STAGE 2: Project to 2D space (same as transform_with_model)
    let embedding_dim = fitted_projections.shape()[1];
    let mut transformed = Array2::zeros((n_samples, embedding_dim));

    for (i, neighbor_indices) in _initial_neighbors.iter().enumerate() {
        let mut weighted_position = Array1::<f64>::zeros(embedding_dim);
        let mut total_weight = 0.0;

        for &neighbor_idx in neighbor_indices.iter().take(model.config.n_neighbors) {
            if neighbor_idx < fitted_projections.shape()[0] {
                let orig_dist = euclidean_distance(normalized_data.row(i), original_data.row(neighbor_idx));
                let weight = 1.0 / (orig_dist + 1e-8);

                for j in 0..embedding_dim {
                    weighted_position[j] += weight * fitted_projections[[neighbor_idx, j]];
                }
                total_weight += weight;
            }
        }

        if total_weight > 0.0 {
            for j in 0..embedding_dim {
                transformed[[i, j]] = weighted_position[j] / total_weight;
            }
        }
    }

    // STAGE 3: Optimization loop to refine the initial projections
    vprint!("[SEARCH] Transform STAGE 3: Optimizing projected positions with gradient descent");

    let optimized_transformed = optimize_transform_positions(
        &mut transformed,
        &normalized_data,
        &original_data,
        fitted_projections,
        &model.config,
        &_initial_neighbors
    )?;

    // STAGE 4: Find final neighbors in the 2D embedding space
    let final_neighbors = find_neighbors_in_embedding_space(&optimized_transformed, fitted_projections, model.config.n_neighbors, model)?;

    vprint!("[OK] Transform with neighbors completed: {} points projected with optimization and 2D neighbors", n_samples);

    Ok((optimized_transformed, final_neighbors))
}

#[no_mangle]
pub extern "C" fn pacmap_fit_transform_quantized(
    data: *const f64,
    rows: usize,
    cols: usize,
    n_dims: usize,
    n_neighbors: usize,
    seed: u64,
) -> *mut f16 {
    // SAFETY: Caller must ensure data points to a valid slice of length rows*cols
    let data_vec = unsafe { std::slice::from_raw_parts(data, rows * cols) }.to_vec();
    let data_arr = Array2::from_shape_vec((rows, cols), data_vec).expect("Invalid shape");
    let config = Configuration {
        embedding_dimensions: n_dims,
        override_neighbors: Some(n_neighbors),
        seed: Some(seed),
        ..Default::default()
    };
    let (embedding_f16, _model) = fit_transform_quantized(data_arr, config).expect("Fit failed");
    // Convert Vec<f16> to raw pointer for C caller
    let (vec, _offset) = embedding_f16.into_raw_vec_and_offset();
    let boxed = vec.into_boxed_slice();
    Box::into_raw(boxed) as *mut f16
}

#[no_mangle]
pub extern "C" fn pacmap_save_model(model: *mut PaCMAP, path: *const u8, path_len: usize) -> i32 {
    let model_ref = unsafe { &mut *model };
    let path_slice = unsafe { std::slice::from_raw_parts(path, path_len) };
    let path_str = std::str::from_utf8(path_slice).expect("Invalid UTF-8 path");
    // Save without quantization by default
    model_ref.quantize_on_save = false;
    match model_ref.save_uncompressed(path_str) {
        Ok(()) => 0,
        Err(_) => 1,
    }
}

#[no_mangle]
pub extern "C" fn pacmap_save_model_quantized(
    model: *mut PaCMAP,
    path: *const u8,
    path_len: usize,
    quantize: bool,
) -> i32 {
    let model_ref = unsafe { &mut *model };
    model_ref.quantize_on_save = quantize;
    let path_slice = unsafe { std::slice::from_raw_parts(path, path_len) };
    let path_str = std::str::from_utf8(path_slice).expect("Invalid UTF-8 path");
    match model_ref.save_compressed(path_str) {
        Ok(()) => 0,
        Err(_) => 1,
    }
}

#[no_mangle]
pub extern "C" fn pacmap_load_model(path: *const u8, path_len: usize) -> *mut PaCMAP {
    let path_slice = unsafe { std::slice::from_raw_parts(path, path_len) };
    let path_str = std::str::from_utf8(path_slice).expect("Invalid UTF-8 path");
    match PaCMAP::load_compressed(path_str) {
        Ok(model) => Box::into_raw(Box::new(model)),
        Err(_) => std::ptr::null_mut(),
    }
}

#[no_mangle]
pub extern "C" fn pacmap_free_model(model: *mut PaCMAP) {
    if !model.is_null() {
        unsafe {
            let _ = Box::from_raw(model);
        }
    }
}

#[no_mangle]
pub extern "C" fn pacmap_free_f16(ptr: *mut f16, len: usize) {
    if !ptr.is_null() {
        unsafe {
            let _ = Box::from_raw(std::slice::from_raw_parts_mut(ptr, len));
        }
    }
}

#[no_mangle]
pub extern "C" fn pacmap_get_knn_indices(
    data: *const f64,
    rows: usize,
    cols: usize,
    n_neighbors: usize,
    seed: u64,
) -> *mut usize {
    let data_vec = unsafe { std::slice::from_raw_parts(data, rows * cols) }.to_vec();
    let data_arr = Array2::from_shape_vec((rows, cols), data_vec).expect("Invalid shape");
    let indices = get_knn_indices(data_arr.view(), n_neighbors, seed);
    let flat: Vec<usize> = indices.into_iter().flatten().collect();
    let boxed = flat.into_boxed_slice();
    Box::into_raw(boxed) as *mut usize
}

#[no_mangle]
pub extern "C" fn pacmap_free_usize(ptr: *mut usize, len: usize) {
    if !ptr.is_null() {
        unsafe {
            let _ = Box::from_raw(std::slice::from_raw_parts_mut(ptr, len));
        }
    }
}

#[no_mangle]
pub extern "C" fn pacmap_distance_stats(
    embedding: *const f64,
    rows: usize,
    cols: usize,
    _k: usize,
    mean: *mut f64,
    p95: *mut f64,
    max: *mut f64,
) {
    let emb_vec = unsafe { std::slice::from_raw_parts(embedding, rows * cols) }.to_vec();
    let emb_arr = Array2::from_shape_vec((rows, cols), emb_vec).expect("Invalid shape");
    let (m, p, mx) = compute_distance_stats(&emb_arr, 42); // Use default seed for FFI consistency
    unsafe {
        *mean = m;
        *p95 = p;
        *max = mx;
    }
}

#[no_mangle]
pub extern "C" fn pacmap_get_model_stats(
    model: *const PaCMAP,
    mean: *mut f64,
    p95: *mut f64,
    max: *mut f64,
) {
    let model_ref = unsafe { &*model };
    unsafe {
        *mean = model_ref.stats.mean_distance;
        *p95 = model_ref.stats.p95_distance;
        *max = model_ref.stats.max_distance;
    }
}

/// Symmetrize k-NN graph to improve connectivity and reduce artifacts
/// Makes the graph undirected: if i is neighbor of j, ensure j is neighbor of i
/// Symmetrize per-point neighbor lists with proper distance-based selection
/// Ensures fixed size n_neighbors per point using distance-weighted merging
fn symmetrize_per_point(nn_per_point: &mut Vec<Vec<(usize, f64)>>, n_neighbors: usize) {
    let n_samples = nn_per_point.len();

    // Store original neighbors before symmetrization (for isolated point fallback)
    let original_neighbors = nn_per_point.clone();

    // Collect all bidirectional connections with minimum distance
    let mut bidirectional: std::collections::HashMap<(usize, usize), f64> = std::collections::HashMap::new();

    for i in 0..n_samples {
        for &(j, dist) in &nn_per_point[i] {
            let key = if i < j { (i, j) } else { (j, i) };
            bidirectional.entry(key).and_modify(|existing_dist| {
                *existing_dist = existing_dist.min(dist); // Use minimum distance
            }).or_insert(dist);
        }
    }

    // Rebuild neighbor lists ensuring symmetry and minimum neighbor count for isolated points
    for i in 0..n_samples {
        let mut all_candidates: Vec<(usize, f64)> = Vec::new();

        // Collect all valid symmetric neighbors for point i
        for (edge, &dist) in &bidirectional {
            let (a, b) = *edge;
            if a == i {
                all_candidates.push((b, dist));
            } else if b == i {
                all_candidates.push((a, dist));
            }
        }

        // Sort by distance and take exactly n_neighbors (or all available if less)
        all_candidates.sort_by(|x, y| x.1.partial_cmp(&y.1).unwrap_or(std::cmp::Ordering::Equal));

        // FIXED: Handle isolated points by maintaining minimum neighbor count
        if all_candidates.len() < n_neighbors {
            // Point is isolated - add closest asymmetric neighbors to reach n_neighbors
            vprint!("WARNING: Point {} has only {} symmetric neighbors, adding {} asymmetric neighbors",
                   i, all_candidates.len(), n_neighbors - all_candidates.len());

            // Get original neighbors for this point (before symmetrization)
            let point_original_neighbors = &original_neighbors[i];
            let mut asymmetric_candidates: Vec<(usize, f64)> = Vec::new();

            // Find neighbors that aren't already in symmetric list
            let symmetric_indices: std::collections::HashSet<usize> = all_candidates.iter().map(|(idx, _)| *idx).collect();
            for &(j, dist) in point_original_neighbors {
                if !symmetric_indices.contains(&j) {
                    asymmetric_candidates.push((j, dist));
                }
            }

            // Sort and add the closest asymmetric neighbors
            asymmetric_candidates.sort_by(|x, y| x.1.partial_cmp(&y.1).unwrap_or(std::cmp::Ordering::Equal));
            all_candidates.extend(asymmetric_candidates.into_iter().take(n_neighbors - all_candidates.len()));
        }

        nn_per_point[i] = all_candidates.into_iter().take(n_neighbors).collect();
    }

    if std::env::var("PACMAP_VERBOSE").is_ok() {
        let total_pairs = bidirectional.len() * 2; // Each bidirectional edge = 2 directed pairs
        eprintln!("[SYNC] SYMMETRIC PER-POINT: {} bidirectional edges â†’ {} total directed pairs",
                 bidirectional.len(), total_pairs);
    }
}

/// Find neighbors for new points in the original high-dimensional space
/// Uses direct KNN only for <1k samples or when forced, otherwise uses HNSW with NO fallback
/// Returns Vec<Vec<usize>> where each inner Vec contains neighbor indices
fn find_neighbors_in_original_space(
    new_data: &Array2<f64>,
    original_data: &Array2<f64>,
    k: usize,
    model: &PaCMAP
) -> Result<Vec<Vec<usize>>, Box<dyn std::error::Error>> {
    let (n_new, _n_features) = new_data.dim();
    let (n_orig, _) = original_data.dim();

    let mut all_neighbors = Vec::with_capacity(n_new);

    // CRITICAL: Use direct KNN only for <1k samples OR when forced, otherwise HNSW with NO fallback
    let should_use_direct_knn = n_orig < 1000 || model.config.force_knn;

    if should_use_direct_knn {
        vprint!("[SEARCH] Transform: Using direct KNN for neighbor search ({} points < 1k or forced)", n_orig);

        for i in 0..n_new {
            let mut distances: Vec<(usize, f64)> = Vec::with_capacity(n_orig);

            // Calculate distances to all original points
            for j in 0..n_orig {
                let dist = euclidean_distance(new_data.row(i), original_data.row(j));
                distances.push((j, dist));
            }

            // Sort by distance and take top k
            distances.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));
            let neighbors: Vec<usize> = distances.into_iter()
                .take(k.min(n_orig))
                .map(|(idx, _)| idx)
                .collect();

            all_neighbors.push(neighbors);
        }

        return Ok(all_neighbors);
    }

    // For >=1k samples and not forced: MUST use HNSW - NO fallback allowed
    #[cfg(feature = "use_hnsw")]
    {
        // Try to use stored serialized index first (preferred for performance)
        if let Some(ref serialized_index) = model.serialized_hnsw_index {
            vprint!("[SEARCH] Transform: Using stored HNSW index for neighbor search in original space ({} points)", n_orig);

            // Validate CRC32 checksum if available
            if let Some(expected_crc) = model.hnsw_index_crc32 {
                let mut hasher = Hasher::new();
                hasher.update(serialized_index);
                let actual_crc = hasher.finalize();
                if actual_crc != expected_crc {
                    return Err(format!("CRC checksum mismatch for original HNSW index: expected {}, got {}", expected_crc, actual_crc).into());
                }
                vprint!("[OK] Original HNSW index CRC validation passed");
            }

            let deserialized = deserialize_hnsw_from_bytes(serialized_index)?;
            let hnsw = load_hnsw_from_serialized(&deserialized)?;

            // Search for neighbors of each new point
            for i in 0..n_new {
                let query: Vec<f32> = new_data.row(i).iter().map(|&x| x as f32).collect();
                let neighbors = hnsw.search(&query, k, model.config.hnsw_params.ef_search);
                let neighbor_indices: Vec<usize> = neighbors.into_iter().map(|n| n.d_id).collect();
                all_neighbors.push(neighbor_indices);
            }

            vprint!("[OK] Transform: Stored HNSW neighbor search completed");
            return Ok(all_neighbors);
        }

        // If no stored index, rebuild HNSW (should not happen in production)
        vprint!("[WARNING] Transform: No stored HNSW index, rebuilding for neighbor search in original space ({} points)", n_orig);

        let hnsw_params = &model.config.hnsw_params;
        let max_layer = ((n_orig as f32).log2() * 0.8) as usize;
        let max_layer = max_layer.min(32).max(4);

        let hnsw = Hnsw::<f32, DistL2>::new(
            hnsw_params.m,
            n_orig,
            max_layer,
            hnsw_params.ef_construction,
            DistL2{}
        );

        // Insert original data points (convert to f32)
        for (i, row) in original_data.axis_iter(ndarray::Axis(0)).enumerate() {
            let point: Vec<f32> = row.iter().map(|&x| x as f32).collect();
            hnsw.insert((&point, i));
        }

        // Search for neighbors of each new point
        for i in 0..n_new {
            let query: Vec<f32> = new_data.row(i).iter().map(|&x| x as f32).collect();
            let neighbors = hnsw.search(&query, k, hnsw_params.ef_search);
            let neighbor_indices: Vec<usize> = neighbors.into_iter().map(|n| n.d_id).collect();
            all_neighbors.push(neighbor_indices);
        }

        vprint!("[OK] Transform: Rebuilt HNSW neighbor search completed");
        return Ok(all_neighbors);
    }

    #[cfg(not(feature = "use_hnsw"))]
    {
        return Err("Transform failed: Dataset has >=1k samples and requires HNSW, but HNSW feature is not enabled. Use direct KNN for smaller datasets.".into());
    }
}

/// Find neighbors for new points in the 2D embedding space using HNSW
/// Uses stored embedding index when available, otherwise rebuilds HNSW
fn find_neighbors_in_embedding_space(
    new_projections: &Array2<f64>,
    fitted_projections: &Array2<f64>,
    k: usize,
    model: &PaCMAP
) -> Result<Vec<Vec<usize>>, Box<dyn std::error::Error>> {
    let (n_new, _) = new_projections.dim();
    let (n_fitted, _) = fitted_projections.dim();

    let mut all_neighbors = Vec::with_capacity(n_new);

    vprint!("[SEARCH] Finding neighbors in 2D embedding space for {} new points among {} fitted points", n_new, n_fitted);
    vprint!("[INFO] Using HNSW with model parameters: M={}, ef_search={}",
            model.config.hnsw_params.m, model.config.hnsw_params.ef_search);

    // ALWAYS use HNSW in 2D embedding space for consistency
    #[cfg(feature = "use_hnsw")]
    {
        // REMOVED: Never use stored embedding index - always rebuild for accuracy
        // This ensures we always use exact fitted projections rather than approximations

        // ALWAYS rebuild embedding HNSW from exact fitted projections for accuracy
        vprint!("[REBUILDING] Transform: Building embedding HNSW index from exact fitted projections ({} points)", n_fitted);

        let hnsw_params = &model.config.hnsw_params;
        let max_layer = ((n_fitted as f32).log2() * 0.8) as usize;
        let max_layer = max_layer.min(32).max(4);

        let hnsw = Hnsw::<f32, DistL2>::new(
            hnsw_params.m,
            n_fitted,
            max_layer,
            hnsw_params.ef_construction,
            DistL2{}
        );

        // Insert fitted projections (convert to f32)
        for (i, row) in fitted_projections.axis_iter(ndarray::Axis(0)).enumerate() {
            let point: Vec<f32> = row.iter().map(|&x| x as f32).collect();
            hnsw.insert((&point, i));
        }

        // Search for neighbors of each new projection
        for i in 0..n_new {
            let query: Vec<f32> = new_projections.row(i).iter().map(|&x| x as f32).collect();
            let neighbors = hnsw.search(&query, k, hnsw_params.ef_search);
            let neighbor_indices: Vec<usize> = neighbors.into_iter().map(|n| n.d_id).collect();
            all_neighbors.push(neighbor_indices);
        }

        vprint!("[OK] Rebuilt embedding HNSW neighbor search completed");
        return Ok(all_neighbors);
    }

    #[cfg(not(feature = "use_hnsw"))]
    {
        return Err("HNSW feature not enabled but required for 2D neighbor search".into());
    }
}

/// Calculate Euclidean distance between two array views
fn euclidean_distance(a: ndarray::ArrayView1<f64>, b: ndarray::ArrayView1<f64>) -> f64 {
    a.iter()
        .zip(b.iter())
        .map(|(x, y)| (x - y).powi(2))
        .sum::<f64>()
        .sqrt()
}

/// Transforms new data and returns detailed statistics for each point.
/// This is the primary function for AI applications to detect out-of-distribution data.
/// Implements "No Man's Land" detection for identifying outliers, adversarial inputs, or data drift.
pub fn transform_with_stats(model: &mut PaCMAP, new_data: Array2<f64>) -> Result<Vec<TransformStats>, Box<dyn std::error::Error>> {
    let (n_samples, _n_features) = new_data.dim();

    // Step 1: Get the optimized coordinates using the existing, correct transform function
    vprint!("[SEARCH] Computing optimized transform for {} new points", n_samples);
    let optimized_embedding = transform_with_model(model, new_data)?;

    // Step 2: Use stored HNSW indices for fast outlier statistics computation
    vprint!("[INFO] Calculating No Man's Land statistics using stored HNSW indices");

    // Ensure the embedding HNSW index is available BEFORE borrowing other data
    if model.config.used_hnsw {
        #[cfg(feature = "use_hnsw")]
        {
            model.ensure_embedding_hnsw_index()?;
        }
    }

    // Now get the immutable references
    let centroid = model.embedding_centroid.as_ref().ok_or("Model does not have an embedding centroid. Was it fitted correctly?")?;
    let _fitted_projections = &model.fitted_projections; // Always stored now

    let stats_results: Vec<TransformStats> = if model.config.used_hnsw {
        #[cfg(feature = "use_hnsw")]
        {

            if let Some(ref embedding_hnsw) = model.embedding_hnsw_index {
                // FAST PATH: Use stored HNSW index for embedding space neighbor search
                vprint!("[FAST] Using stored embedding HNSW index for fast neighbor statistics");

                (0..n_samples).map(|i| {
                    let point_coords = optimized_embedding.row(i);
                    let query: Vec<f32> = point_coords.iter().map(|&x| x as f32).collect();

                    // Find the single closest neighbor using HNSW
                    let closest_neighbors = embedding_hnsw.search(&query, 1, model.config.hnsw_params.ef_search);
                    let distance_to_closest_neighbor = if let Some(closest) = closest_neighbors.first() {
                        (closest.distance as f64).sqrt() // Convert from squared distance
                    } else {
                        f64::INFINITY
                    };

                    // Find k neighbors for mean distance using HNSW
                    let k_neighbors = embedding_hnsw.search(&query, model.config.n_neighbors, model.config.hnsw_params.ef_search);
                    let mean_distance_to_k_neighbors = if !k_neighbors.is_empty() {
                        k_neighbors.iter().map(|n| (n.distance as f64).sqrt()).sum::<f64>() / k_neighbors.len() as f64
                    } else {
                        f64::INFINITY
                    };

                    // Calculate distance to training centroid
                    let distance_to_training_centroid = point_coords.iter().zip(centroid.iter())
                        .map(|(p, c)| (p - c).powi(2))
                        .sum::<f64>()
                        .sqrt();

                    TransformStats {
                        coordinates: point_coords.to_owned(),
                        distance_to_closest_neighbor,
                        mean_distance_to_k_neighbors,
                        distance_to_training_centroid,
                    }
                }).collect()
            } else {
                return Err("Model was trained with HNSW but embedding index is missing".into());
            }
        }
        #[cfg(not(feature = "use_hnsw"))]
        {
            return Err("HNSW feature not enabled, cannot use stored indices for stats".into());
        }
    } else {
        // Graceful error for models not trained with HNSW
        return Err("Model was not trained with HNSW indices. Cannot compute fast outlier statistics. Please retrain the model with HNSW enabled.".into());
    };

    vprint!("[OK] No Man's Land analysis completed for {} points", n_samples);

    // Report summary statistics for outlier detection
    if is_verbose() {
        let closest_distances: Vec<f64> = stats_results.iter().map(|s| s.distance_to_closest_neighbor).collect();
        let centroid_distances: Vec<f64> = stats_results.iter().map(|s| s.distance_to_training_centroid).collect();

        let min_closest = closest_distances.iter().copied().fold(f64::INFINITY, f64::min);
        let max_closest = closest_distances.iter().copied().fold(f64::NEG_INFINITY, f64::max);
        let min_centroid = centroid_distances.iter().copied().fold(f64::INFINITY, f64::min);
        let max_centroid = centroid_distances.iter().copied().fold(f64::NEG_INFINITY, f64::max);

        eprintln!("[INFO] OUTLIER DETECTION SUMMARY:");
        eprintln!("   Distance to closest neighbor: min={:.3}, max={:.3}", min_closest, max_closest);
        eprintln!("   Distance to training centroid: min={:.3}, max={:.3}", min_centroid, max_centroid);
        eprintln!("   Points with distance_to_closest > 2.0 (potential outliers): {}",
                 closest_distances.iter().filter(|&&d| d > 2.0).count());
    }

    Ok(stats_results)
}


/// Format duration in human-readable format
fn format_duration(duration: std::time::Duration) -> String {
    let total_secs = duration.as_secs();
    let millis = duration.subsec_millis();

    if total_secs >= 60 {
        let mins = total_secs / 60;
        let secs = total_secs % 60;
        format!("{}m{}s", mins, secs)
    } else if total_secs >= 1 {
        if millis > 0 {
            format!("{}.{}s", total_secs, millis / 100)
        } else {
            format!("{}s", total_secs)
        }
    } else {
        format!("{}ms", millis)
    }
}